{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunal/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "import operator\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Environment Parameters\n",
    "GRID_HEIGHT = 4 \n",
    "GRID_WIDTH = 12\n",
    "TIME_PENALTY = -1\n",
    "FALL_PENALTY = -100\n",
    "START = (1,1)\n",
    "GOAL1 = (GRID_WIDTH, 1)\n",
    "GOAL2 = (GRID_WIDTH, GRID_HEIGHT)\n",
    "GOALS = [GOAL1]\n",
    "ACTIONS = [\n",
    "    (0, 1),   #UP\n",
    "    (0, -1),  #DOWN\n",
    "    (1, 0),   #RIGHT\n",
    "    (-1, 0),  #LEFT\n",
    "#     (1, 1),   #NORTH_EAST\n",
    "#     (-1, 1),  #NORTH_WEST\n",
    "#     (1, -1),  #SOUTH_EAST\n",
    "#     (-1, -1)  #SOUTH_WEST\n",
    "]\n",
    "ACTION_PLOT = {\n",
    "    (0, 1) : u'\\u2191',\n",
    "    (0, -1) : u'\\u2193',\n",
    "    (1, 0) : u'\\u2192',\n",
    "    (-1, 0) : u'\\u2190',\n",
    "#     (1, 1) : u'\\u2197',\n",
    "#     (-1, 1) :  u'\\u2196',\n",
    "#     (1, -1) :  u'\\u2198',\n",
    "#     (-1, -1) : u'\\u2199',\n",
    "    None : '-'\n",
    "}\n",
    "STATE_LISTS = [range(1, GRID_WIDTH + 1), range(1, GRID_HEIGHT + 1)]\n",
    "STATES = list(itertools.product(*STATE_LISTS))\n",
    "NUM_EPISODES = 500\n",
    "# Epsilon for epsilon greedy agent.\n",
    "EPSILON = 0.1\n",
    "ALPHA = 0.2\n",
    "GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_valid_state(state):\n",
    "    x, y = state[0], state[1]\n",
    "    return x in range(1, GRID_WIDTH + 1) and y in range(1, GRID_HEIGHT + 1)\n",
    "\n",
    "def is_start_state(state):\n",
    "    assert is_valid_state(state)\n",
    "    return state == START\n",
    "\n",
    "def is_goal_state(state):\n",
    "    assert is_valid_state(state)\n",
    "    return state in GOALS\n",
    "\n",
    "def is_cliff_state(state):\n",
    "    assert is_valid_state(state)\n",
    "    x, y = state[0], state[1]\n",
    "    return (y != GRID_HEIGHT) and (((x - y) == 1) or ((x + y) == 7))\n",
    "#     return (y == 1) and (not is_start_state(state)) and (not is_goal_state(state))\n",
    "\n",
    "def is_terminal_state(state):\n",
    "    return is_goal_state(state) or is_cliff_state(state)\n",
    "\n",
    "# NOTE : This can return invalid states if action is invalid.\n",
    "def get_next_state(state, action):\n",
    "    if is_terminal_state(state):\n",
    "        return state\n",
    "    return tuple(map(operator.add, state, action))\n",
    "\n",
    "def get_valid_actions(state):\n",
    "    # There are no actions in terminal states. We return all actions since reward for terminal states is 0\n",
    "    # and get_next_state is designed to not make any transitions in terminal states.\n",
    "    if is_terminal_state(state):\n",
    "        return ACTIONS\n",
    "    valid_actions = []\n",
    "    for action in ACTIONS:\n",
    "        if is_valid_state(get_next_state(state, action)):\n",
    "            valid_actions.append(action)\n",
    "    return valid_actions\n",
    "\n",
    "def act(state, action):\n",
    "    assert not is_terminal_state(state)\n",
    "    next_state = get_next_state(state, action)\n",
    "    if is_cliff_state(next_state):\n",
    "        reward = FALL_PENALTY\n",
    "    elif is_goal_state(next_state):\n",
    "        if next_state == GOAL1:\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = 0\n",
    "    else:\n",
    "        reward = TIME_PENALTY\n",
    "    return next_state, reward\n",
    "\n",
    "def init_q(Q):\n",
    "    for state in STATES:\n",
    "        valid_actions = get_valid_actions(state)\n",
    "        for action in valid_actions:\n",
    "            next_state = get_next_state(state, action)\n",
    "            if is_valid_state(next_state):\n",
    "                Q[state][action] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_random_action(state):\n",
    "    # For non-terminal states there is atleast one valid action.\n",
    "    # For terminal states it doesn't matter what action we return since all have reward 0.    \n",
    "    valid_actions = get_valid_actions(state)\n",
    "    assert len(valid_actions) > 0\n",
    "    return random.choice(valid_actions)\n",
    "\n",
    "def get_greedy_action(Q, state):\n",
    "    return max(Q[state], key=Q[state].get)\n",
    "\n",
    "def get_epsilon_greedy_action(Q, state):\n",
    "    # Choose an exploratory action with probability epsilon\n",
    "    if random.random() < EPSILON:\n",
    "#         print('Returning Random Choice')\n",
    "        return get_random_action(state)\n",
    "    # Return greedy action\n",
    "    return get_greedy_action(Q, state)\n",
    "\n",
    "def get_optimal_policy(Q, state):\n",
    "    optimal_policy = {} \n",
    "    visited = {}\n",
    "    while not is_terminal_state(state):\n",
    "        optimal_action = get_greedy_action(Q, state)\n",
    "        optimal_policy[state] = optimal_action\n",
    "        visited[state] = True\n",
    "        state = get_next_state(state, optimal_action)\n",
    "        if state in visited:\n",
    "            break\n",
    "    return optimal_policy\n",
    "\n",
    "def plot_policy(policy):\n",
    "    for y in reversed(xrange(1, GRID_HEIGHT + 1)):\n",
    "        print('')\n",
    "        for x in xrange(1, GRID_WIDTH + 1):\n",
    "            state = (x,y)\n",
    "            if is_start_state(state):\n",
    "                char = 'S'\n",
    "            elif is_goal_state(state):\n",
    "                if state == GOAL1:\n",
    "                    char = 'G1'\n",
    "                else:\n",
    "                    char = 'G2'\n",
    "            elif is_cliff_state(state):\n",
    "                char = 'C'\n",
    "            else:\n",
    "                action = policy[state] if state in policy else None\n",
    "                char = (ACTION_PLOT[action])\n",
    "            print('%s\\t' %(char), end='')\n",
    "\n",
    "def get_smoothed_list(list_to_smooth, window_size = 10):\n",
    "    num_episodes = len(list_to_smooth)\n",
    "    smoothed_list = []\n",
    "    cur_sum = sum(list_to_smooth[:window_size])\n",
    "    smoothed_list.append(cur_sum * 1.0 / window_size)\n",
    "    for i in xrange(window_size, num_episodes):\n",
    "        cur_sum -= list_to_smooth[i-window_size]\n",
    "        cur_sum += list_to_smooth[i]\n",
    "        smoothed_list.append(cur_sum * 1.0 / window_size)\n",
    "    return smoothed_list\n",
    "\n",
    "def plot_smoothed_list(list1, list2, xlabel, ylabel, legend_labels):\n",
    "    plt.plot(get_smoothed_list(list1))\n",
    "    plt.plot(get_smoothed_list(list2))\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(legend_labels, loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def td_update(Q, state, action, reward, next_state, td_type='SARSA'):\n",
    "    next_action = get_epsilon_greedy_action(Q, next_state)\n",
    "    if td_type == 'SARSA':\n",
    "        update_action = next_action\n",
    "    elif td_type == 'QLEARNING':\n",
    "        update_action = get_greedy_action(Q, next_state)\n",
    "    else:\n",
    "        raise Exception('Invalid TD Type %s' % (td_type))\n",
    "#             print('State:{0} Action:{1} Reward{2} NextState:{3} NextAction{4}'.format(state, action, reward,\n",
    "#                                                                                       next_state, next_action))\n",
    "    Q[state][action] += ALPHA * (reward + GAMMA * Q[next_state][update_action] - Q[state][action])\n",
    "    return next_state, next_action\n",
    "\n",
    "def dyna_q_update(Q, observed_state_actions, planning_steps, td_type):\n",
    "    for step in range(planning_steps):\n",
    "        state = random.choice(observed_state_actions.keys())\n",
    "        action = random.choice(list(observed_state_actions[state]))\n",
    "        next_state, reward = act(state, action)\n",
    "        _, _ = td_update(Q, state, action, reward, next_state, td_type)\n",
    "        \n",
    "def td_learning(num_episodes, td_type='SARSA', planning_steps=0):\n",
    "    Q = defaultdict(dict)\n",
    "    init_q(Q)\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    if planning_steps != 0:\n",
    "        observed_state_actions = {}\n",
    "    for episode in xrange(num_episodes):\n",
    "#         print('Episode {0}'.format(episode))\n",
    "        state = START\n",
    "        action = get_epsilon_greedy_action(Q, state)\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        while not is_terminal_state(state):\n",
    "            next_state, reward = act(state, action)\n",
    "            episode_reward += reward\n",
    "            state, action = td_update(Q, state, action, reward, next_state, td_type)\n",
    "            if planning_steps != 0:\n",
    "                if state not in observed_state_actions:\n",
    "                    observed_state_actions[state] = set([action])\n",
    "                else:\n",
    "                    observed_state_actions[state].add(action)\n",
    "                dyna_q_update(Q, observed_state_actions, planning_steps, td_type)\n",
    "            steps += 1\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(steps)\n",
    "    optimal_policy = get_optimal_policy(Q, START)\n",
    "    return episode_rewards, episode_steps, optimal_policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sarsa_rewards, sarsa_steps, sarsa_policy, sarsa_Q = td_learning(NUM_EPISODES, 'SARSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-\t-\t-\t-\t-\t"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(1, -1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ce090ec5eb04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msarsa_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d9e346b5e41b>\u001b[0m in \u001b[0;36mplot_policy\u001b[0;34m(policy)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mACTION_PLOT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s\\t'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (1, -1)"
     ]
    }
   ],
   "source": [
    "plot_policy(sarsa_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qlearning_rewards, qlearning_steps, qlearning_policy, qlearning_Q = td_learning(NUM_EPISODES, 'QLEARNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_policy(qlearning_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_smoothed_list(sarsa_rewards, qlearning_rewards, 'Episode', 'Rewards', ['SARSA', 'QLearning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_smoothed_list(sarsa_steps, qlearning_steps, 'Episode', 'Steps', ['SARSA', 'QLearning'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
